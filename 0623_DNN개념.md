# DNN 핵심개념



\* (CodeLab) Your first Keras model, with transfer learning @ http://j.mp/2XAb8tt
\* 사전 학습된 ConvNet을 이용한 전이 학습 (Tensorflow Official) @ https://j.mp/3bK0PXA

\* model interpretability vs model explainability @ https://j.mp/2AUy4df
\* LIME (python library for model explanation & interpretable model) @ https://goo.gl/5JcZFY
\* 정형데이터를 위한 인공신경망 모델, TabNet (Google Cloud AI, feature selection & interpretability) @ https://j.mp/38ABU8e & https://bit.ly/3kFdlNU
\* 데이터 라벨링 너무 귀찮아요: 컨센서스 라벨링 도입기 (DEVIEW2020) @ https://j.mp/38AYUnC

**[선대 미적분]**

Essence of linear algebra @ https://goo.gl/O79Wsx
Essence of calculus @ https://goo.gl/3KHqk6

**[data engineer]**

\* 데이터과학자와 데이터엔지니어를 위한 인터뷰 문답집 (책, 매우 세부적인 MLDL 관련 Q&A) @ https://j.mp/2ZX7wAV

\* Statistics 110 : Probability from Harvard (커넥트 재단, 한글) @ https://j.mp/3uhcOVp

**[논문]**

\* 데이터과학자와 데이터엔지니어를 위한 인터뷰 문답집 (책, 매우 세부적인 MLDL 관련 Q&A) @ https://j.mp/2ZX7wAV

[10:26](https://w1647847118-lq5111919.slack.com/archives/C03KK7CFU4C/p1655947601275889)

\* PR12 (Youtube, 논문읽기모임) @ https://j.mp/2FDR6Xz
\* Papers You Must Read (by 고려대 Data Science & Business Analytics 연구실)@ https://j.mp/3259sIy
\* 세미나 모음자료 by 고려대 DSBA 연구실 (Paper Review & Lecture Review) @ https://bit.ly/3s7ubcF



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623103319356.png" alt="image-20220623103319356" style="zoom:50%;" />



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623091835905.png" alt="image-20220623091835905" style="zoom:50%;" />

- ANN의 넓이: 퍼셉트론 갯수 (많은단계)
- ANN의 깊이: layer 의 갯수 (많은 레이어)



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623093242762.png" alt="image-20220623093242762" style="zoom:50%;" />



- Fine tuning



## ML vs DNN

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623095817593.png" alt="image-20220623095817593" style="zoom:50%;" />





<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623095900502.png" alt="image-20220623095900502" style="zoom:50%;" />

- Feature Extraction : 기존의 열들을 바탕으로 새로운 열을 만들어준다,데이터의 특징을 찾아냄 (PCA등)



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623100755909.png" alt="image-20220623100755909" style="zoom:50%;" />





## (single-layer) perceptron

단층 퍼센트론

- 뉴런을 본따 만든 알고리즘 **하나의 단위**
- 2가지연산을 적용해 출력계산 
  - 1. 넘겨져 온 데이터와 세타들의 Linear combination 을 계산 (합&곱)
    2. 

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623103711671.png" alt="image-20220623103711671" style="zoom:50%;" />

> W0 = theta0 임 (가중치)

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623103757175.png" alt="image-20220623103757175" style="zoom:50%;" />

> 선형결합
>
> 싹다 곱하고 더하면 나옴

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623104022041.png" alt="image-20220623104022041" style="zoom:50%;" />





### Activition Function

![image-20220623113552395](/Users/krc/Library/Application Support/typora-user-images/image-20220623113552395.png)

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623104207664.png" alt="image-20220623104207664" style="zoom:50%;" />

- Bias: 고정치

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623104554447.png" alt="image-20220623104554447" style="zoom:50%;" />

> g(x) 라는 껍데기 함수 없다고 생각하고
>
> 곱하고 더하고 
>
> w , bias -> theta임 : 상수
>
> a, b,c :상수
>
> 

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623110314207.png" alt="image-20220623110314207" style="zoom:50%;" />



#### 동그란모양으로

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623110412788.png" alt="image-20220623110412788" style="zoom:50%;" />

- 비선형함수 : 선형이아닌 모든것

- 레이어과정을 거칠때마다 함수 그래프 꺽어짐(비선형으로)



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623111054280.png" alt="image-20220623111054280"  />

> 시그모이드 3.8일때 -> step Function 1 ?





<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623111247186.png" alt="image-20220623111247186" style="zoom:50%;" />

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623111631992.png" alt="image-20220623111631992" style="zoom:50%;" />

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623112542416.png" alt="image-20220623112542416" style="zoom:50%;" />

> **한단계 뒤로**가는것은 **퍼셉트론 통째로 미분**
>
> 퍼셉트론은 실제로 껍데기 함수g(x)를 미분하는것
>
> 



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623111658499.png" alt="image-20220623111658499" style="zoom:50%;" />

> 0은 미분 못함 -> 기울기 언제나 0 나옴
>
> 나머지는 접선의 수직이 되는건가?
>
> 로지스틱의 시그모이드 함수(0과 1사이에 있는값 양수만 나온다) 접선의 기울기가 0이아님
>
> 

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623112255724.png" alt="image-20220623112255724" style="zoom:50%;" />

> Tanh 를 쓰면 음수의 값도 나온다



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623112432955.png" alt="image-20220623112432955" style="zoom:50%;" />

> 시그모이드의 미분의 최대값은 0.25로 한정되어 있다



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623112730219.png" alt="image-20220623112730219" style="zoom:50%;" />

> 렐루 함수
>
> 미분값 (기울기)가 0 또는 1로만 출력



![image-20220623113138924](/Users/krc/Library/Application Support/typora-user-images/image-20220623113138924.png)

![image-20220623113311200](/Users/krc/Library/Application Support/typora-user-images/image-20220623113311200.png)

> Leaky ReLU : 범위를 벗어난 렐루함수를 보완시킴
>
> 알파를 아주 작은값으로 0.01 0.001



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623113518876.png" alt="image-20220623113518876" style="zoom:50%;" />

#### xor

: 두개의 입력으로 0또는1이 출력 (1일때 1)

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623113926551.png" alt="image-20220623113926551" style="zoom:50%;" />

> 퍼셉트론 하나짜리로 풀어보기
>
> 계단함수사용 
>
> 

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623113942520.png" alt="image-20220623113942520" style="zoom:50%;" />

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623114143248.png" alt="image-20220623114143248" style="zoom:50%;" />

![image-20220623114305058](/Users/krc/Library/Application Support/typora-user-images/image-20220623114305058.png)



> 1. Xor 한다
> 2. Step F 0.2 정도로 잡는다.(or문제) / and 일경우 0.7 로함
>     
>    1. 선형결합 x 곱하기 theta 

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623114428013.png" alt="image-20220623114428013" style="zoom:50%;" />

> 선형결합된것이 x 



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623114441860.png" alt="image-20220623114441860" style="zoom:50%;" />



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623114532716.png" alt="image-20220623114532716" style="zoom:50%;" />

> and 연산 0.7인경우



> Class 0 class1 
>
> XOR은 선형분리불가능

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623114740903.png" alt="image-20220623114740903" style="zoom:50%;" />



## 다층 퍼셉트론 MLP multi-layer perceptron

:복수의 퍼셉트론을 연결한 구조

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623115150290.png" alt="image-20220623115150290" style="zoom:50%;" />

> x1에 not을 때린다
>
> x2에 not을 때린다
>
> 각 경우 And and 
>
> 하고 마지막에 or 때림



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623115255107.png" alt="image-20220623115255107" style="zoom:50%;" />

> 오른쪽으로 갈수록 오버피팅위험이 크다
>
> 딥러닝은 오버피팅을 얼마나 걷어내는가 관건



## 인공신경망 ANN

![image-20220623120247727](/Users/krc/Library/Application Support/typora-user-images/image-20220623120247727.png)

> x데이터는 총 3개의 열로 이루어져있다
>
> 총 100개의 행
>
> 집 크기/ 높이 / 연식
>
> Y : 가격이 평균보다 이상이면1 아니면 0 
>
> 이때 x데이터는 크기가 100행 3열 행렬 
>
> 인공신경망 들어갈때 90도 눕힌다 ==> [100*3] 100행3열
>
> 인공신경망 [3*4] 3행4열
>
> 퍼셉트론 첫번째 연산 : 행렬곱 (싹다 곱하고 싹다 더하기)
>
> [100*3] * [3*4] => [100 * 4] ( 노드하나당 100개씩 들어감) 
>
> 모든 인공신경망 거쳐서 계산함 최종=> [100* 2]
>
> 최종값 소프트맥스 적용
>
> 소프트맥스 거쳐도 차원바뀌지않음
>
> 퍼셉트론 거칠때마다 g(x)라는 activty함수 씌워짐 (차원에 영향 없음)
>
> 정답 Y 기준을  벡터형식으로 01011110101 출력 one hot label 이진분류 한다.
>
> [100행 2열] 짜리가 됨
>
> 소프트맥스 적용값과 원핫레이블 끼리 크로스 엔트로피
>
> 크로스엔트로피로 모델평가

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623115833122.png" alt="image-20220623115833122" style="zoom:50%;" />



#### DNN: hidden layer 2개이상

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623120412194.png" alt="image-20220623120412194" style="zoom:50%;" />



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623120929608.png" alt="image-20220623120929608" style="zoom:50%;" />



## Back propagation algorithm 

## 오차역전파 알고리즘 == 신경망의 효율적인 학습방법

:( gradient decent 준비과정)

: 



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220623121423913.png" alt="image-20220623121423913" style="zoom:50%;" />

* PReLU :parametric ReLU  가장진화된 렐루