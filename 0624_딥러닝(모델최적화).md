



# 딥러닝 모델 최적화 이론



### Neural Network Optimization

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220624100618989.png" alt="image-20220624100618989" style="zoom:50%;" />

gradient descent 를활용한 신경망의 학습과정



1. #### Weight lnitialization 

- gradient descent 를 적용하기위한 첫 단계는 모든 **파라미터 세타를 초기화**하는것 (랜덤하게)
- 

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220624093417148.png" alt="image-20220624093417148" style="zoom:50%;" />



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220624093425175.png" alt="image-20220624093425175" style="zoom:50%;" />



> theta값을 다르게하면 출발지점이 달라질수있다



- perceptron의 Linear combination 결과값 (activation function으로의 입력 값)이 너무 커지거나 작아지지 않게 만들어주려는것이 핵심
- 발전된 초기화 방법들을 활용해 vanishing gradient혹은 exploding gradient문제를 줄일수있음
  - 자비에 초기화

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220624094037670.png" alt="image-20220624094037670" style="zoom:50%;" />



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220624094801073.png" alt="image-20220624094801073" style="zoom:50%;" />



> 1번 히든레이어 2번 레이어있다 하나의 퍼센트론
>
> 세타 여러개있는데 how 세터 초기화? 할수있는가
>
> 정규분포로부터 세타값을 뽑아낼것
>
> 노드의 수가 n개 
>
> 퍼셉트론 2개있다고 가정함 -> 루트1/2 정규분포되는것임
>
> 노드 100개 -> 루트 1/100 =>1/10=> 0.1 
>
> activate function했을때 너무크지않게 세타를 초기화(값을 줄여주는)해주던 것임
>
> 앞선 레이어의 퍼셉트론수가 많을수록 곱해지는 세타값이 작다

- 
  - 헤(흐어) 초기화

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220624094903926.png" alt="image-20220624094903926" style="zoom:50%;" />



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220624095057620.png" alt="image-20220624095057620" style="zoom:50%;" />

> x축:gradient decent 횟수 





#### Gradient descent algorithm 경사하강법

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220624100835686.png" alt="image-20220624100835686" style="zoom:50%;" />

> 낮을수록 좋지만 오버피팅 조심해야한다



2. #### Weight regularization 가중치 규제법

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220624101203783.png" alt="image-20220624101203783" style="zoom:50%;" />

> 0이 아닌값으로 
>
> 세타의 절대값이 커진다
>
> 모델이 복잡할수록 기울기 커지고 세타값이 커진다
>
> 



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220624101446173.png" alt="image-20220624101446173" style="zoom:50%;" />

![image-20220624102308395](/Users/krc/Library/Application Support/typora-user-images/image-20220624102308395.png)



> 람다 =1 이라고 가정
>
> L1 절대값 취해주고 시그마
>
> L2 제곱 시그마



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220624101748374.png" alt="image-20220624101748374" style="zoom:50%;" />

>어떤 버젼을 사용해도 모델이 복잡해지면 세타가 커진다





<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220624102017930.png" alt="image-20220624102017930" style="zoom:50%;" />



> MSE+ 규제( 커지는 세타 방지용/ 오버피팅을 전반적으로 줄여줌)



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220624102259780.png" alt="image-20220624102259780" style="zoom:50%;" />

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220624105725734.png" alt="image-20220624105725734" style="zoom:50%;" />

> **L2: Ridge** (왼) [regression에서 소수 피쳐만 중요하지는 않다] 오버피팅을 전반적으로 줄여준다 

> **L1: Lasso** 올가미 (오) [regression 에서 소수 피쳐만 중요하다]

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220624103017898.png" alt="image-20220624103017898" style="zoom:50%;" />

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220624103141954.png" alt="image-20220624103141954" style="zoom:50%;" />

>세타값이 파란 원밖으로 나갈수 없다
>
>세타값들이 이동된다 (세타값 너무커지지않게 조정됨)



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220624103332021.png" alt="image-20220624103332021" style="zoom:50%;" />

![image-20220624103742638](/Users/krc/Library/Application Support/typora-user-images/image-20220624103742638.png)



> 절대값그래프 (사각형) 에서 벗어나지않으면서 세타 조정됨
>
> 세타1이 0 으로 되면서 세터1과 곱해지던 x1이 0으로 되어버림 
>
> ==> 즉 **feature selection 효과** 해당 **특성을 배제**하는데 도움됨
>
> **-12 x1** => **0**

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220624104822220.png" alt="image-20220624104822220" style="zoom:50%;" />

\* L1 정규화 및 L2 정규화 비교 & 가중치 변화 시각화 @ https://goo.gl/qa8Xes

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220624104937103.png" alt="image-20220624104937103" style="zoom:50%;" />

> 모델이 단순=> 복잡으로 되서 세타가 10정도 증가 했을때
>
> **람다를 적게**주면 세타값 신경을 덜 쓴다 **언더피팅,모델에 더 신경쓴다** 
>
> **람다크면** 세타값 신경쓰고 **오버피팅에 중점**을 둔다 (오버피팅 빡세게 누른다)



3. #### Advanced gradient descent algorithm



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220624111727286.png" alt="image-20220624111727286" style="zoom:50%;" />

> 기존의 gradient descent (1000개 데이터 다넣고 1발자국)와 다르게 
>
> 랜덤하게 데이터 1개 넣고 한발자국가고 1개넣고 한발자국간다
>
> SGD regressor
>
> SGD classifier

~~~python
from sklearn import linear_model
model = linear_model.SGDRegressor()
~~~





<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220624112200350.png" alt="image-20220624112200350" style="zoom:50%;" />

> 1000개중 지정한 100개 넣고 gradient descent한번 나아가기 반복
>
> 





Full-batch : 그냥 gradient descent

Mini-batch ; 표준방식



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220624112715082.png" alt="image-20220624112715082" style="zoom:50%;" />

> Gradient descent 1회 = 1 iteration



- gradient descent를 대신 적용시켜주는 **optimizer**

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220624112916198.png" alt="image-20220624112916198" style="zoom: 67%;" />



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220624113439973.png" alt="image-20220624113439973" style="zoom:50%;" />

> [Adam, RMSprop, Adagrad 세가지는 돌려봐라!]



\* 각 Optmizer의 Python code 구현 @ http://j.mp/2Y0jWcl
\* Optimizers - EXPLAINED (Youtube video) @ https://j.mp/3bnoRu0
\* 각 Optimizer 들의 수식 @ https://goo.gl/RxXwYn & http://j.mp/2NIou33
\* Learning Rate Schedules and Adaptive Learning Rate Methods for Deep Learning @ https://goo.gl/NYs2j9

[11:33](https://w1647847118-lq5111919.slack.com/archives/C03KK7CFU4C/p1656038005431989)

\* Image source : 백날 자습해도 이해 안 가던 딥러닝, 머리 속에 인스톨 시켜드립니다 by 하용호 @ https://goo.gl/3ZWpnU

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220624113834898.png" alt="image-20220624113834898" style="zoom:50%;" />

> Learning rate decay (schedule)
>
> Adam



\* AdaBelief (Adapting stepsizes by the belief in observed gradients) @ https://j.mp/3iQEBaL & https://j.mp/2M83z9t
\* AdaBound (using dynamic bound of learning rate) @ http://j.mp/2VAo3Lz & http://j.mp/2VENjR1 (http://j.mp/2Q4Iv1c)
\* RAdam (Rectified-Adam) @ http://j.mp/2P5OmF3 & http://j.mp/2qzRjUa & http://j.mp/2N322hu (http://j.mp/2MYtPQ2)



