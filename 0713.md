0713_딥러닝

# 지도학습 비지도학습



#### supervised learning: Label (답지)바탕으로 model이 학습을 한다

#### Unsupervised learning : dataset = data+label (레이블링 되지않은 데이터셋으로 학습/레이블링 안해서 비용 절감)

 

**Recognition**

- Object가 어떤 것인지 구분합니다 

**Object Detection**

- Recognition보다 더 작은 범위로써 Object의 존재 유무만 판단합니다.

Object Detection Algorithms에 대해서 대략적으로 요약하면 다음과 같이 작동합니다.

- 전처리 (Pre-processing)
- 특징 추출 (Feature Extraction)
- 분류 (Classifier)



K-means Clustering 

k=2 일때 (군집개수)

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220713143410779.png" alt="image-20220713143410779" style="zoom:50%;" />

> 기준 새로 세워지며 군집 반복한다



#### Semi-supervised learning : super+unsuper {대표적으로 Pseudo-labeling} 

>  레이블링된 데이터셋으로 모델 지도학습한다 
>
> 데이터셋 500개중 실제 사용 데이터는 300개로 (**test set** 100개 +**validation**100 개 날리므로)=> 성능 70프로 나온다고 가정할때
>
> 수도(가짜)레이블링 사용 :레이블이 없는 데이터셋을 사용하는것
>
> 데이터셋을 확보하고 모델(성능70프로짜리)이 하는 예측(레이블생성됨) confidence
>
> 데이터 마다 confidence 확인해서 성능높으면 데이터셋 새로구축하고
>
> 성능낮으면 지운다
>
> 성능이 좋은 데이터를 기존의 데이터셋에 붙여서 1500개의 데이터로 모델성능을 이끌어 낼수있다.

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220713144658167.png" alt="image-20220713144658167" style="zoom:50%;" />



# 선형회귀



Linear + Regression 

선을 그어 + 값을 예측



Regression : 집값 예측 / classfication : 개,고양이 분류



1. 데이터를 일반화하는 선을 찾는다. 

> x 에서 값이 들어와서 y로 예측하는것

2. 그 선을 이용해 값을 예측한다.

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220713145706175.png" alt="image-20220713145706175" style="zoom:50%;" />



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220713151403877.png" alt="image-20220713151403877" style="zoom:50%;" />

> 건축연도는 숫자가 크므로 scaling 사용한다

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220713151412213.png" alt="image-20220713151412213" style="zoom:50%;" />

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220713151501866.png" alt="image-20220713151501866" style="zoom:50%;" />

> x1가 증가할수록 H도 증가하므로 w1의 부호는 **+** 이다
>
> x3가 감소할수록 H도 증가하므로 w3의 부호는 **-** 이다
>
> > H(x)= w1x1 +b (bias)





#### bias 편향

오른쪽이 더 예측력이 좋다 . 좋은성능. {실제값과 예측값의 차이가 적다}

왼쪽은 bias가 0으로 고정되어있어 데이터의 분포를 충분히 담지못했다.



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220713161312630.png" alt="image-20220713161312630" style="zoom:50%;" />



# 회귀직선

H(x)=Wx+b

Hypothesis 예측값



> x가 11이라고 했을때 예측값과 실제 값(정답:label)의 차이를 error 라 한다

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220713161737626.png" alt="image-20220713161737626" style="zoom:50%;" />

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220713161930333.png" alt="image-20220713161930333" style="zoom:50%;" />

## 비용함수 cost function

**Cost 가 가장작게하는 직선을 찾는다.**(선형회귀에서)



직선상위 y값이 예측값

데이터(점)위 값이 실제값(정답)





<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220713162233640.png" alt="image-20220713162233640" style="zoom:50%;" />



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220713162142787.png" alt="image-20220713162142787" style="zoom:50%;" />





## 입력과 출력

각 특성에 가중치를 곱하고 그것의 가중합(+ bias) 하는 구조   => 선형회귀

가중치의 조합으로 서로다른 가중합 계산을 하고 즉 서로다른 **특성**을 추출한다



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220713163736864.png" alt="image-20220713163736864" style="zoom:50%;" />

# 신경망



### 신경망 모델



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220713164521156.png" alt="image-20220713164521156" style="zoom:50%;" />

## 선형변환

 두 선형변환을 하나의 선형으로 변환



5번 선형변환 + 5번 + 1 번 선형변환 =총 11번 선형변환

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220713171539624.png" alt="image-20220713171539624" style="zoom:50%;" />

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220713171554265.png" alt="image-20220713171554265" style="zoom:50%;" />

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220713171613695.png" alt="image-20220713171613695" style="zoom:50%;" />

한번의 레이어로도 같은결과 **h(x)**

여러층을 쌓아도 의미 없으므로

이것을 막기위해 비선형변환사용한다



## 비선형변환

활성화함수 (activation functoin) : sigmoid ReLU . . . 등등

#### sigmoid

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220713172042288.png" alt="image-20220713172042288" style="zoom:33%;" />

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220713172027814.png" alt="image-20220713172027814" style="zoom:50%;" />

> x가 **-무한**대로갔을때 분모가 엄청커지므로 
>
> Sigmoid(x)-> **0**으로 수렴한다
>
> x가 **무한** 갈수록 e가 0에 수렴하므로 1/1+0 남는다
>
> Sigmoid(x)->**1**	





시그모이드 --> 스위치처럼 임계값이 넘으면 1 출력 안넘으면 출력안함 0 

비선형변환을 끼워넣어 깊은층들을활용할수있고 정확한 추측할수있다.



선형변환은 선형 변환을해도 길이가 같지만

비선형은 길이가 같지않다

<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220713191326743.png" alt="image-20220713191326743" style="zoom:50%;" />

### ReLU

장점 : 계산 단순, gradient vanishing 문제를 해결함



<img src="/Users/krc/Library/Application Support/typora-user-images/image-20220713191439319.png" alt="image-20220713191439319" style="zoom:50%;" />

<img src="/Users/krc/Documents/TIL/image-20220714131210686.png" alt="image-20220714131210686" style="zoom:50%;" />



# 가중치 행렬



#### 데이터 행렬

<img src="/Users/krc/Documents/TIL/image-20220714131622951.png" alt="image-20220714131622951" style="zoom:50%;" />

> **One-hot encoding:** 한 특성을 여러 특성으로 나눔
>
> 서울: 0 **one-hot encoding ->**(1,0,0)	 집의위치_서울
>
> 광역시:1 								**->** (0,1,0) 	집의위치_광역시
>
> 지방:2									**->** (0,0,1) 	집의위치_지방





- 행렬곱

<img src="/Users/krc/Documents/TIL/image-20220714132358862.png" alt="image-20220714132358862" style="zoom:50%;" />



<img src="/Users/krc/Documents/TIL/image-20220714132535767.png" alt="image-20220714132535767" style="zoom:50%;" />

<img src="/Users/krc/Documents/TIL/image-20220714132622331.png" alt="image-20220714132622331" style="zoom:50%;" />

![image-20220714134218132](/Users/krc/Documents/TIL/image-20220714134218132.png)

- 데이터 행렬에 행추가

데이터개수에 따라 row개수 달라짐



- 가중치 행렬에 열추가

가중치에따라 결과의 column열이 추가된다.



<img src="/Users/krc/Documents/TIL/image-20220714134533261.png" alt="image-20220714134533261" style="zoom:50%;" /><img src="/Users/krc/Documents/TIL/image-20220714134750000.png" alt="image-20220714134750000" style="zoom:50%;" />

<img src="/Users/krc/Documents/TIL/image-20220714134548550.png" alt="image-20220714134548550" style="zoom:50%;" />



# 경사하강법



## 비용함수

Convex 

minimum 을 구하는 함수

한정된시간으로 어떤 방향이 내리막길인지 찾아야함



<img src="/Users/krc/Documents/TIL/image-20220714134921099.png" alt="image-20220714134921099" style="zoom:50%;" />



- 비용함수 가중치 업데이트

비용을 줄이는 방향을 선택

<img src="/Users/krc/Documents/TIL/image-20220714135125316.png" alt="image-20220714135125316" style="zoom:50%;" />



cost를 줄이는 방향을 알수있는 가장 명확한 방법은 기울기를 구하는것이다

기울기의 반대 부호의 방향으로 이동하면 더 낮은 cost로 이동할수있다



<img src="/Users/krc/Documents/TIL/image-20220714135232025.png" alt="image-20220714135232025" style="zoom:50%;" />

<img src="/Users/krc/Documents/TIL/image-20220714135237993.png" alt="image-20220714135237993" style="zoom:50%;" />



> New W = 기존 w -[기울기 반대 방향이므로 - ] * alpha [학습률]* 기울기[cost/w] [편미분]
>
> 기울기가 클 경우 많이 움직인다



- 학습률 learning rate  lr

<img src="/Users/krc/Documents/TIL/image-20220714140022565.png" alt="image-20220714140022565" style="zoom:50%;" />




기울기 즉 알파값(학습율)이 너무크면 오히려 최적에서 멀어지므로

학습률을 작게해서 최적에 도달할수있게한다

하지만 현실적으로 성능이 떨어져 힘들경우도 많으니 알잘딱깔센 학습률조정

(alpha)학습율이 **높을수록 보폭크다** / <u>낮을수록 보폭작다</u>



-  편미분

**Partial** derivative : **부분**에만 미분을 하겠다 

이 식에서 x2는 상수로 생각하고 (= 신경쓰지않겠다는 의미) 

2 x1 만 미분 진행 ==> 2



<img src="/Users/krc/Documents/TIL/image-20220714143040960.png" alt="image-20220714143040960" style="zoom:50%;" />

<img src="/Users/krc/Documents/TIL/image-20220714143401931.png" alt="image-20220714143401931" style="zoom:50%;" />





# Optimizer 종류

### Gradient Descent

<img src="/Users/krc/Documents/TIL/image-20220714144111327.png" alt="image-20220714144111327" style="zoom:50%;" />

기울기 반대 방향이므로 (-) 부호



### SGD : stochastic Gradient Descent	 확률적

- **Batch GD** : 배치 [데이터셋 전체 갯수]

전체 데이터행렬의 오차를 다 더해서 **cost**를 구하고 업데이트를 진행하는 것



- **Mini-batch GD**/ single sample GD

샘플링 한 데이터의 error 다 더해서 **loss**를 구하고 업데이트 진행



 [ Cost vs Loss ]

: 배치 vs 미니배치 차이





### Momentum

SGD+Momentum(관성) 

: SGD에 관성값을 추가함

<img src="/Users/krc/Documents/TIL/image-20220714145235020.png" alt="image-20220714145235020" style="zoom:50%;" />



### AdaGrad

<img src="/Users/krc/Documents/TIL/image-20220714151824983.png" alt="image-20220714151824983" style="zoom:50%;" />

SDG+adaptive 학습률 감소 (learning rate decay)

학습률 앞에 **새로운값을 곱해줌**으로써 전체 항의 값을 조절한다.

- **RMSprop** : AdaGrad + Gradient decay

앞에 곱해주는 값이 제곱합의 역수로 곱해주는 형태이기 때문에  분모가 무한이되면 => 0 이 되는데 

w'= w-ㅁ 에서 ㅁ가 0 이면 더이상 w'가 바뀌지 않으므로

gradient 값을 감소시켜주는 decay값을 포함시킨다.

<img src="/Users/krc/Documents/TIL/image-20220714152040574.png" alt="image-20220714152040574" style="zoom:50%;" />



### Adam

**Ada**grad(RMSprop) +**M**omentum

[adamW, AdamP ....]





# 역전파

<img src="/Users/krc/Documents/TIL/image-20220714153650717.png" alt="image-20220714153650717" style="zoom:50%;" />

<img src="/Users/krc/Documents/TIL/image-20220714153725713.png" style="zoom:50%;" />



### 연쇄법칙



<img src="/Users/krc/Documents/TIL/image-20220714154640885.png" alt="image-20220714154640885" style="zoom:50%;" />

<img src="/Users/krc/Documents/TIL/image-20220714154725992.png" alt="image-20220714154725992" style="zoom:50%;" />

<img src="/Users/krc/Documents/TIL/image-20220714154843566.png" alt="image-20220714154843566" style="zoom:50%;" />

> A가 증가할때 C가 증가하는 순간변화량(기울기) 
>
> = (B가 증가할때 C가 증가하는 변화량) *( A가 증가할때 B가 증가하는 변화량)



<img src="/Users/krc/Documents/TIL/image-20220714160159367.png" alt="image-20220714160159367" style="zoom:50%;" />

~~~python
net=Net() # 네트워크를 오브젝트로 만든다
output=net(input) # 네트워크에 input이 있을때 통과시켜서 ouput 을 나오게한다
loss=criterion(output,target)  # loss구하기
# input에서 어떤타겟 y와 output 을 비교해서 crition(loss함수임) 어떤방식으로 차이를 계산할것인지 결정
loss.backward() #역전파
~~~





### ReLU 미분

<img src="/Users/krc/Documents/TIL/image-20220714163036520.png" alt="image-20220714163036520" style="zoom:50%;" />

> RelU 변화량 ( x 가 변화했을때 출력값의 변화량 )
>
> x가 0보다클때 변화량 :1
>
> x가 0보다 작을때 변화량:0

<img src="/Users/krc/Documents/TIL/image-20220714163647010.png" alt="image-20220714163647010" style="zoom:50%;" />

> 신경망 거칠때 
>
> 가중치 곱하고 결과값나옴
>
> 결과값 z1을 비선형함수 h1 (ReLU) 사용하여 최종 결과값 z1나옴
>
> 이과정도 변화되는 과정(렐루변환)이므로 기울기 구해서 역전파에 포함을 시켜야한다 => 편미분z1/편미분h1



### backpropagation

기울기 저장해놓고 저장해놓은값 재사용



# Soft max



- 예측값을 확률값으로 변환하는 함수
- 확률이기때문에 확률값 다 더하면 1
- 음수 & divide by zero 막기위해서 자연상수의 지수사용

<img src="/Users/krc/Documents/TIL/image-20220714173549527.png" alt="image-20220714173549527" style="zoom:50%;" />

<img src="/Users/krc/Documents/TIL/image-20220714173558958.png" alt="image-20220714173558958" style="zoom:50%;" />



# Cross Entropy

불확실성

- Infomation

<img src="/Users/krc/Documents/TIL/image-20220714174323248.png" alt="image-20220714174323248" style="zoom:50%;" /><img src="/Users/krc/Documents/TIL/image-20220714174029230.png" alt="image-20220714174029230" style="zoom:50%;" />

> 인풋 0일때(사건의 발생확률이 적을수록) I 가 무한대로  (information이 많아진다)
>
> 인풋 1 일때(사건의 발생확률이 높을수록) I가 0으로 (information이 0이다)



- Entropy:  

Average information

<img src="/Users/krc/Documents/TIL/image-20220714175111174.png" alt="image-20220714175111174" style="zoom:50%;" />

> I = log p(x) : information 해당사건이 일어날 확률을 로그와 - 부호붙여준것
>
> H = 해당 사건확률 * I
>
> H= IA + IB

<img src="/Users/krc/Documents/TIL/image-20220714175301628.png" alt="image-20220714175301628" style="zoom:50%;" />

> 엔트로피 1 
>
> 엔트로피 0.81



